{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rezaahmadi-99/drl-in-hft/blob/main/final_drl_in_hft.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "raxn9tM2TxOh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.stats import linregress\n",
        "import math\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils import weight_norm\n",
        "from torch.distributions.categorical import Categorical\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "import scipy\n",
        "from scipy.stats import norm\n",
        "from warnings import filterwarnings\n",
        "filterwarnings('ignore')\n",
        "import sys"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGR9SJvQkrTR"
      },
      "source": [
        "# **Data Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jCjWc48pT1_5"
      },
      "outputs": [],
      "source": [
        "data = pd.read_pickle('/content/GARAN.E_snapshots_200ms.xz')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaPGH3oNUNoP"
      },
      "outputs": [],
      "source": [
        "order_book = {timestamp: entry['Orderbook'] for timestamp, entry in data.items()}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnWdGWOcUPnk"
      },
      "outputs": [],
      "source": [
        "data_10_to_6 = []\n",
        "for d in list(data.items()):\n",
        "  if d[0] >= pd.Timestamp('2018-10-08 10:00:00.000000+0300', tz='Europe/Istanbul') and d[0] < pd.Timestamp('2018-10-08 18:00:00.000000+0300', tz='Europe/Istanbul'):\n",
        "    data_10_to_6.append(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cg-YeR9jURbh"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.75 * len(data_10_to_6))\n",
        "train_data = data_10_to_6[:train_size]\n",
        "test_data = data_10_to_6[train_size:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJ9aLR4AUTT9"
      },
      "outputs": [],
      "source": [
        "# data_names = ['Orderbook', 'Executed', 'Added', 'Deleted']\n",
        "orderbook_train, executed_train, added_train, deleted_train = [], [], [], []\n",
        "for data in train_data:\n",
        "  orderbook_train.append(data[1]['Orderbook'].tolist())\n",
        "  executed_train.append(data[1]['Executed'])\n",
        "  added_train.append(data[1]['Added'])\n",
        "  deleted_train.append(data[1]['Deleted'])\n",
        "\n",
        "orderbook_test, executed_test, added_test, deleted_test = [], [], [], []\n",
        "for data in test_data:\n",
        "  orderbook_test.append(data[1]['Orderbook'].tolist())\n",
        "  executed_test.append(data[1]['Executed'])\n",
        "  added_test.append(data[1]['Added'])\n",
        "  deleted_test.append(data[1]['Deleted'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60QjBptRUVQx"
      },
      "outputs": [],
      "source": [
        "def transform_exe_add_del(input_data):\n",
        "  input_transformed = []\n",
        "\n",
        "  for ex in input_data:\n",
        "    buy_quantity_sum = 0\n",
        "    buy_weighted_price_sum = 0\n",
        "    sell_quantity_sum = 0\n",
        "    sell_weighted_price_sum = 0\n",
        "    for i in ex:\n",
        "      if i['side'] == 'buy':\n",
        "        buy_weighted_price_sum += i['price'] * i['quantity']\n",
        "        buy_quantity_sum += i['quantity']\n",
        "      else:\n",
        "        sell_weighted_price_sum += i['price'] * i['quantity']\n",
        "        sell_quantity_sum += i['quantity']\n",
        "    if buy_quantity_sum == 0:\n",
        "      buy_weighted_avg_price = 0\n",
        "    else:\n",
        "      buy_weighted_avg_price = round(buy_weighted_price_sum / buy_quantity_sum, 3)\n",
        "    if sell_quantity_sum == 0:\n",
        "      sell_weighted_avg_price = 0\n",
        "    else:\n",
        "      sell_weighted_avg_price = round(sell_weighted_price_sum / sell_quantity_sum, 3)\n",
        "\n",
        "    input_transformed.append([buy_weighted_avg_price, buy_quantity_sum, sell_weighted_avg_price, sell_quantity_sum])\n",
        "\n",
        "  return input_transformed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E10Tz_sVUXdW"
      },
      "outputs": [],
      "source": [
        "ex_train_transformed = torch.tensor(transform_exe_add_del(executed_train)).float()\n",
        "ex_test_transformed = torch.tensor(transform_exe_add_del(executed_test)).float()\n",
        "orderbook_train_top = torch.tensor([each[0] for each in orderbook_train])\n",
        "orderbook_test_top = torch.tensor([each[0] for each in orderbook_test])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ld1aTXwXSM_"
      },
      "source": [
        "# **Data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Of8LFmUo6uF3"
      },
      "outputs": [],
      "source": [
        "orderbook_train_sac = torch.tensor(orderbook_train).reshape(108000, 1, 10, 4).float()\n",
        "ex_train_transformed_sac = ex_train_transformed.reshape(108000, 4).float()\n",
        "prices_train = orderbook_train_top[:, 1:3].float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sp66q9N-Syg3"
      },
      "outputs": [],
      "source": [
        "orderbook_test_sac = torch.tensor(orderbook_test).reshape(36000, 1, 10, 4).float()\n",
        "ex_test_transformed_sac = ex_test_transformed.reshape(36000, 4).float()\n",
        "prices_test = orderbook_test_top[:,1:3].float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aU2BtRHJVfn2"
      },
      "source": [
        "Seeing if there are any empty orderbook values in train and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BStlsDJRU8yB"
      },
      "outputs": [],
      "source": [
        "print(f\"Any zero orderbooks in the training data:\")\n",
        "print(torch.all(orderbook_train_sac == 0, dim=3).any())\n",
        "print(f\"\\nAny zero orderbooks in the test data:\")\n",
        "print(torch.all(orderbook_test_sac == 0, dim=3).any())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWmQnoGaz6qp"
      },
      "source": [
        "Find zero indices and remove them from training orderbook, execitions and prices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2aGrflTL1Xqn"
      },
      "outputs": [],
      "source": [
        "mask = torch.all(orderbook_train_sac != 0, dim=3)[:, 0, 0]\n",
        "orderbook_train_sac_nonzero = orderbook_train_sac[mask]\n",
        "ex_train_transformed_sac_nonzero = ex_train_transformed_sac[mask]\n",
        "prices_train_nonzero = prices_train[mask]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "WVA70o3d0CCd"
      },
      "outputs": [],
      "source": [
        "print(orderbook_train_sac_nonzero.shape)\n",
        "print(ex_train_transformed_sac_nonzero.shape)\n",
        "print(prices_train_nonzero.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8HLwJ4jb6ru9"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(len(prices_train_nonzero)), prices_train_nonzero[:,0])\n",
        "plt.title('Training Set Price Change in Day')\n",
        "plt.xlabel('time')\n",
        "plt.xticks([])\n",
        "plt.ylabel('price')\n",
        "# plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z334_YXs8x2p"
      },
      "source": [
        "# **Test data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "suuMkoss80YO"
      },
      "outputs": [],
      "source": [
        "orderbook_test_sac = torch.tensor(orderbook_test).reshape(36000, 1, 10, 4).float()\n",
        "ex_test_transformed_sac = ex_test_transformed.reshape(36000, 4).float()\n",
        "prices_test = orderbook_test_top[:,1:3].float()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9Dd6FTRMWQz"
      },
      "outputs": [],
      "source": [
        "print(orderbook_test_sac.shape)\n",
        "print(ex_test_transformed_sac.shape)\n",
        "print(prices_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5VDJUfeo9V92"
      },
      "outputs": [],
      "source": [
        "mid_prices_test = (prices_test[:, 1] + prices_test[:, 0]) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7d_yU9tr9hQk"
      },
      "outputs": [],
      "source": [
        "plt.plot(mid_prices_test)\n",
        "plt.title('Test Set Price Change in Day')\n",
        "plt.xlabel('time')\n",
        "plt.ylabel('price')\n",
        "plt.xticks([])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "der6WbRYds3I"
      },
      "source": [
        "# **Model Development**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKMrpFrKaMkQ"
      },
      "source": [
        "#### **State transformation (feature engineering)**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "compute volume imbalance in the orderbook to consider as a feature"
      ],
      "metadata": {
        "id": "u6aBW6OgYrN7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "olGz8bpQZSxL"
      },
      "outputs": [],
      "source": [
        "def state_func(orderbooks, executions):\n",
        "\n",
        "  v_bid = orderbooks[:, :, :5, 0].sum(dim=-1)\n",
        "  v_ask = orderbooks[:, :, :5, 3].sum(dim=-1)\n",
        "\n",
        "  imbalance_current = (v_bid - v_ask) / (v_bid + v_ask)\n",
        "\n",
        "  ex_qs_current = torch.log1p(executions[:, [1, 3]])\n",
        "\n",
        "  state = torch.concat([imbalance_current, ex_qs_current], dim=-1)\n",
        "\n",
        "  return state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjX6za5PZ6fW"
      },
      "outputs": [],
      "source": [
        "states = state_func(orderbook_train_sac_nonzero, ex_train_transformed_sac_nonzero)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7YjCcLU6FKZC"
      },
      "outputs": [],
      "source": [
        "mid_prices_train_nonzero = (prices_train_nonzero[:, 1] + prices_train_nonzero[:, 0]) / 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgkMH_-n_IN7"
      },
      "outputs": [],
      "source": [
        "plt.plot(mid_prices_train_nonzero)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Analysis of features**"
      ],
      "metadata": {
        "id": "vqffespltvRP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "whole = ex_train_transformed_sac_nonzero[:, [1, 3]]"
      ],
      "metadata": {
        "id": "quc3D9jAdasv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "states[:,1]"
      ],
      "metadata": {
        "id": "zLx-sDRuiRn2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "size, scale = 1000, 10\n",
        "\n",
        "imbalance = pd.Series(states[:, 0])\n",
        "\n",
        "imbalance.plot.hist(grid=True, bins=20, rwidth=0.9,\n",
        "                   color='#607c8e')\n",
        "plt.title('Imbalance Distribution')\n",
        "plt.xlabel('imbalance')\n",
        "plt.ylabel('fequency')\n",
        "plt.grid(axis='y', alpha=0.75)"
      ],
      "metadata": {
        "id": "Uy9sLh0aM0p4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train set executed buy orders distribution"
      ],
      "metadata": {
        "id": "SvVlDgnaZERQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "buys = pd.Series(whole[:, 0])"
      ],
      "metadata": {
        "id": "14e6IMmz-9h3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "buys.plot.hist(grid=True, bins=20, rwidth=0.9,\n",
        "                   color='#607c8e')\n",
        "plt.yscale('log')\n",
        "plt.title('Total Exectued Buy Orders Distribution')\n",
        "plt.xlabel('total executed buy orders')\n",
        "plt.ylabel('$\\log_{10}$fequency')\n",
        "plt.grid(axis='y', alpha=0.75)"
      ],
      "metadata": {
        "id": "TzKkfoyYCmm_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train set executed sell orders distribution"
      ],
      "metadata": {
        "id": "LwZuLGpuZTG5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sells = pd.Series(whole[:, 1])"
      ],
      "metadata": {
        "id": "-yEXXuG0EKH2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sells.plot.hist(grid=True, bins=20, rwidth=0.9,\n",
        "                   color='#607c8e')\n",
        "plt.yscale('log')\n",
        "plt.title('Total Exectued Sell Orders Distribution')\n",
        "plt.xlabel('total executed sell orders')\n",
        "plt.ylabel('$\\log_{10}$fequency')\n",
        "plt.grid(axis='y', alpha=0.75)"
      ],
      "metadata": {
        "id": "E9tsUR6DEJme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdzLaD-4aUGj"
      },
      "source": [
        "#### **Replay buffer**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FRHxt2Im95Ki"
      },
      "outputs": [],
      "source": [
        "class ReplayBufferRaw:\n",
        "\n",
        "  def __init__(self, capacity, env_len):\n",
        "\n",
        "    self.capacity = capacity\n",
        "    self.env_len = env_len\n",
        "    self.ptr = 0 # pointer for circular buffer\n",
        "    self.size = 0 # number of transitions in the buffer\n",
        "    self.states = [None] * capacity # [self.size, env=1, n_features]\n",
        "    self.actions = [None] * capacity # [self.size, env=1]\n",
        "    self.rewards = [None] * capacity  # [self.size, 1]\n",
        "    self.next_states = [None] * capacity # [self.size, env=1, n_features]\n",
        "    self.dones = [None] * capacity # [self.size, env=1]\n",
        "\n",
        "    self.ptr_list = []\n",
        "\n",
        "  def add(self, state, action, next_state, reward, done):\n",
        "\n",
        "    self.states[self.ptr] = state\n",
        "    self.actions[self.ptr] = action\n",
        "    self.next_states[self.ptr] = next_state\n",
        "    self.rewards[self.ptr] = reward\n",
        "    self.dones[self.ptr] = done\n",
        "\n",
        "    self.ptr_list.append(self.ptr)\n",
        "\n",
        "    # update pointer and size\n",
        "    self.ptr = (self.ptr + 1) % self.capacity\n",
        "    self.size = min(self.size + 1, self.capacity)\n",
        "\n",
        "  def normalize_recent_rewards(self):\n",
        "\n",
        "\n",
        "    extracted = torch.tensor((np.array(self.rewards[:self.size])[self.ptr_list]).tolist())\n",
        "\n",
        "    # normalize them\n",
        "\n",
        "    normalized_rewards = torch.sign(extracted) * torch.log1p(torch.abs(extracted))\n",
        "\n",
        "    # mad = torch.median(torch.abs(extracted - torch.median(extracted)))\n",
        "    # normalized_rewards = extracted / (mad + 1e-6)\n",
        "    # print(normalized_rewards)\n",
        "\n",
        "\n",
        "    for i, index in enumerate(self.ptr_list):\n",
        "\n",
        "      # replace them with non-normal values\n",
        "      self.rewards[index] = torch.tensor(normalized_rewards[i])\n",
        "\n",
        "\n",
        "    # at the end empty the list\n",
        "    self.ptr_list = []\n",
        "\n",
        "    return extracted.mean()\n",
        "\n",
        "  def sample(self, n_batches):\n",
        "\n",
        "    sample_indices = np.random.choice(self.size, size=n_batches)\n",
        "\n",
        "    states_return = torch.stack(self.states[:self.size])[sample_indices] # [n_batches, 1, n_features]\n",
        "    actions_return = torch.stack(self.actions[:self.size])[sample_indices] # [n_batches, 1]\n",
        "    next_states_return = torch.stack(self.next_states[:self.size])[sample_indices] # [n_batches, 1, n_features]\n",
        "\n",
        "    mean_rewards_whole_buffer = round(torch.stack(self.rewards[:self.size]).mean().item(), 3)\n",
        "    print(f\"\\nMean rewards of whole buffer: {mean_rewards_whole_buffer}\")\n",
        "\n",
        "    rewards_return = torch.stack(self.rewards[:self.size])[sample_indices] # [n_batches, 1, 1]\n",
        "\n",
        "    # self.dones[-1] = torch.ones(1) # to set the last item of the total buffer to 1 flag\n",
        "\n",
        "    dones_return = torch.stack(self.dones[:self.size])[sample_indices] # [n_batches, 1]\n",
        "\n",
        "    return states_return, actions_return, next_states_return, rewards_return, dones_return, mean_rewards_whole_buffer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Replay buffer for Sharpe ratio reward**"
      ],
      "metadata": {
        "id": "4TiQ4XEN1_YL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sharpe_shaped_rewards(rewards: torch.Tensor, window: int = 20, eps: float = 1e-8):\n",
        "    rewards = rewards.float()\n",
        "    T = rewards.shape[0]\n",
        "\n",
        "    shaped = torch.zeros_like(rewards)\n",
        "\n",
        "    for t in range(T):\n",
        "        # Use expanding window until enough samples are available\n",
        "        start = max(0, t - window + 1)\n",
        "        window_rewards = rewards[start:t+1]\n",
        "\n",
        "        # Compute rolling std\n",
        "        mean = window_rewards.mean()\n",
        "        std = window_rewards.std(unbiased=False)  # population std\n",
        "        if std < eps:\n",
        "            std = eps\n",
        "\n",
        "        # Sharpe-style shaping: mean of return throughout the window/ volatility of the window\n",
        "        shaped[t] = mean / std\n",
        "\n",
        "    return shaped"
      ],
      "metadata": {
        "id": "pqaKdIgiKqBw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayBufferSharpe:\n",
        "\n",
        "  def __init__(self, capacity, env_len):\n",
        "\n",
        "    self.capacity = capacity\n",
        "    self.env_len = env_len\n",
        "    self.ptr = 0 # pointer for circular buffer\n",
        "    self.size = 0 # number of transitions in the buffer\n",
        "    self.states = [None] * capacity # [self.size, env=1, n_features]\n",
        "    self.actions = [None] * capacity # [self.size, env=1]\n",
        "    self.rewards = [None] * capacity  # [self.size, 1]\n",
        "    self.next_states = [None] * capacity # [self.size, env=1, n_features]\n",
        "    self.dones = [None] * capacity # [self.size, env=1]\n",
        "\n",
        "    self.ptr_list = []\n",
        "\n",
        "  def add(self, state, action, next_state, reward, done):\n",
        "\n",
        "    self.states[self.ptr] = state\n",
        "    self.actions[self.ptr] = action\n",
        "    self.next_states[self.ptr] = next_state\n",
        "    self.rewards[self.ptr] = reward\n",
        "    self.dones[self.ptr] = done\n",
        "\n",
        "    self.ptr_list.append(self.ptr)\n",
        "\n",
        "    # update pointer and size\n",
        "    self.ptr = (self.ptr + 1) % self.capacity\n",
        "    self.size = min(self.size + 1, self.capacity)\n",
        "\n",
        "  def normalize_recent_rewards(self):\n",
        "\n",
        "    # print(self.rewards[:self.size])\n",
        "\n",
        "    # extract non-normal values\n",
        "    # print((np.array(self.rewards[:self.size])[self.ptr_list]).tolist())\n",
        "    extracted = torch.tensor((np.array(self.rewards[:self.size])[self.ptr_list]).tolist())\n",
        "\n",
        "    # if Sharpe is applied, normalization should not be done\n",
        "    sharpe_rewards = sharpe_shaped_rewards(extracted, window=500)\n",
        "\n",
        "\n",
        "    for i, index in enumerate(self.ptr_list):\n",
        "\n",
        "      # replace them with original values\n",
        "      self.rewards[index] = torch.tensor(sharpe_rewards[i])\n",
        "\n",
        "\n",
        "    # at the end enpty the list\n",
        "    self.ptr_list = []\n",
        "\n",
        "    return extracted.mean()\n",
        "\n",
        "  def sample(self, n_batches):\n",
        "\n",
        "    sample_indices = np.random.choice(self.size, size=n_batches)\n",
        "\n",
        "    states_return = torch.stack(self.states[:self.size])[sample_indices] # [n_batches, 1, n_features]\n",
        "    actions_return = torch.stack(self.actions[:self.size])[sample_indices] # [n_batches, 1]\n",
        "    next_states_return = torch.stack(self.next_states[:self.size])[sample_indices] # [n_batches, 1, n_features]\n",
        "\n",
        "    mean_rewards_whole_buffer = round(torch.stack(self.rewards[:self.size]).mean().item(), 3)\n",
        "    print(f\"\\nMean rewards of whole buffer: {mean_rewards_whole_buffer}\")\n",
        "\n",
        "    rewards_return = torch.stack(self.rewards[:self.size])[sample_indices] # [n_batches, 1, 1]\n",
        "\n",
        "    # self.dones[-1] = torch.ones(1) # to set the last item of the total buffer to 1 flag\n",
        "\n",
        "    dones_return = torch.stack(self.dones[:self.size])[sample_indices] # [n_batches, 1]\n",
        "\n",
        "    return states_return, actions_return, next_states_return, rewards_return, dones_return, mean_rewards_whole_buffer"
      ],
      "metadata": {
        "id": "uKUAvlt52Hfa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZQ1Bmi9oOAb"
      },
      "source": [
        "#### **SAC Class**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Actor class**"
      ],
      "metadata": {
        "id": "MWHhp3iggYJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Actor(nn.Module):\n",
        "  def __init__(self,in_dim=3, action_dim=3):\n",
        "    super().__init__()\n",
        "    self.in_dim = in_dim\n",
        "    self.action_dim = action_dim\n",
        "    self.linear_1 = nn.Linear(in_dim, 128)\n",
        "    self.linear_2 = nn.Linear(128, 64)\n",
        "    self.linear_3 = nn.Linear(64, 32)\n",
        "    self.linear_4 = nn.Linear(32, action_dim)\n",
        "  def forward(self, state):\n",
        "    x = F.relu(self.linear_1(state))\n",
        "    x = F.relu(self.linear_2(x))\n",
        "    x = F.relu(self.linear_3(x))\n",
        "    logit = F.relu(self.linear_4(x))\n",
        "    probs = F.softmax(logit, -1) # turn each three action values into probabilities\n",
        "    dist = Categorical(probs)\n",
        "    actions = dist.sample()\n",
        "\n",
        "    log_probs = logit - torch.logsumexp(logit, dim=-1)\n",
        "\n",
        "    # logsumexp does the exact same thing as the commented code below\n",
        "    # log_probs = logit - torch.log(torch.sum(torch.exp(logit), dim=-1, keepdim=True))\n",
        "\n",
        "\n",
        "    return actions, probs, log_probs"
      ],
      "metadata": {
        "id": "WetYXrYhgOcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Critic class**"
      ],
      "metadata": {
        "id": "1BkfozBagSVE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "  def __init__(self, in_dim=3, action_dim=3):\n",
        "    super().__init__()\n",
        "    self.in_dim = in_dim\n",
        "    self.action_dim = action_dim\n",
        "    self.linear_1 = nn.Linear(in_dim, 128)\n",
        "    self.linear_2 = nn.Linear(128, 64)\n",
        "    self.linear_3 = nn.Linear(64, 32)\n",
        "    self.linear_4 = nn.Linear(32, action_dim)\n",
        "  def forward(self, state, action):\n",
        "    x = F.relu(self.linear_1(state))\n",
        "    x = F.relu(self.linear_2(x))\n",
        "    x = F.relu(self.linear_3(x))\n",
        "    q_values = F.relu(self.linear_4(x))\n",
        "\n",
        "    # we do not return selected Q-values in discrete SAC unlike continuous SAC\n",
        "    # where action space is infinite and we have to pass the selected Q-values\n",
        "\n",
        "    return q_values"
      ],
      "metadata": {
        "id": "TUl46d5fgQ-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Soft actor-critic class**"
      ],
      "metadata": {
        "id": "nbmM15VuhJlr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m1ou1RioR1A"
      },
      "outputs": [],
      "source": [
        "class SAC(nn.Module):\n",
        "  def __init__(self, tau=0.005, gamma=0.99, actor_lr=1e-4, critics_lr=1e-4, alpha_lr=1e-4):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.tau = tau\n",
        "    self.gamma = gamma\n",
        "\n",
        "    self.actor = Actor()\n",
        "    self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
        "\n",
        "    self.critic1 = Critic()\n",
        "    self.critic1_optimizer = torch.optim.Adam(self.critic1.parameters(), lr=critics_lr)\n",
        "\n",
        "    self.critic2 = Critic()\n",
        "    self.critic2_optimizer = torch.optim.Adam(self.critic2.parameters(), lr=critics_lr)\n",
        "\n",
        "    self.target_critic1 = Critic()\n",
        "    self.target_critic2 = Critic()\n",
        "\n",
        "    self.target_critic1.load_state_dict(self.critic1.state_dict())\n",
        "    self.target_critic2.load_state_dict(self.critic2.state_dict())\n",
        "\n",
        "    self.alpha = torch.tensor(np.log(2.), requires_grad=True)\n",
        "    self.target_entropy = -np.log(3)\n",
        "    self.alpha_optimizer = torch.optim.Adam([self.alpha], lr=alpha_lr)\n",
        "\n",
        "\n",
        "  def forward(self):\n",
        "    pass\n",
        "\n",
        "  def actor_params(self):\n",
        "    return self.actor.state_dict()\n",
        "\n",
        "  def update_parameters(self, current_state, actions, next_state, reward, done):\n",
        "\n",
        "    # Target Q computation\n",
        "\n",
        "    with torch.no_grad():\n",
        "      next_action, next_prob, next_log_prob = self.actor(next_state)\n",
        "\n",
        "      # action shape: [n_batches, 1]\n",
        "      # probability shape: [n_batches, 1, n_features]\n",
        "      # log probability shape: [n_batches, 1, n_features]\n",
        "\n",
        "      target_q1 = self.target_critic1(next_state, next_action) # shape[n_batches, 1, n_features]\n",
        "      target_q2 = self.target_critic2(next_state, next_action) # shape[n_batches, 1, n_features]\n",
        "      target_q = torch.min(target_q1, target_q2) # shape[n_batches, 1, n_features]\n",
        "\n",
        "      # print(f\"reward shape: {reward.shape}\") # shape [n_batches, 1, 1]\n",
        "      # print(f\"done shape: {done.shape}\") # shape [n_batches, 1]\n",
        "      # print(f\"target_q shape: {target_q.shape}\") # shape [n_batches, 1, n_features]\n",
        "      # print(f\"log probability shape: {next_log_prob.shape}\") # shape [n_batches, 1, n_features]\n",
        "\n",
        "\n",
        "      V_target = torch.sum(next_prob * (target_q - self.alpha * next_log_prob), dim=-1, keepdim=True) # shape[n_batches, 1, 1]\n",
        "      y = reward + self.gamma * (1 - done.unsqueeze(-1)) * V_target # shape[n_batches, 1, 1]\n",
        "\n",
        "\n",
        "    # critic loss and update\n",
        "\n",
        "    # since the critic returns all Qs, for computing critic loss, we\n",
        "    # need the selected Qs for the actions\n",
        "\n",
        "    q1 = self.critic1(current_state, actions) # [n_batches, 1, n_features]\n",
        "    q2 = self.critic2(current_state, actions) # [n_batches, 1, n_features]\n",
        "\n",
        "    selected_q1 = torch.gather(q1, -1, actions.unsqueeze(-1)) # [n_batches, 1, 1]\n",
        "    selected_q2 = torch.gather(q2, -1, actions.unsqueeze(-1)) # [n_batches, 1, 1]\n",
        "\n",
        "    critic_loss1 = F.mse_loss(selected_q1, y)\n",
        "    critic_loss2 = F.mse_loss(selected_q2, y)\n",
        "\n",
        "    self.critic1_optimizer.zero_grad()\n",
        "    critic_loss1.backward()\n",
        "    self.critic1_optimizer.step()\n",
        "\n",
        "    self.critic2_optimizer.zero_grad()\n",
        "    critic_loss2.backward()\n",
        "    self.critic2_optimizer.step()\n",
        "\n",
        "\n",
        "    # actor loss and update\n",
        "\n",
        "    new_action, new_prob, new_log_prob = self.actor(current_state)\n",
        "\n",
        "    # action shape: [n_batches, 1]\n",
        "    # probability shape: [n_batches, 1, n_features]\n",
        "    # log probability shape: [n_batches, 1, n_features]\n",
        "\n",
        "    new_q = torch.min(self.critic1(current_state, new_action), self.critic2(current_state, new_action)) # shape[n_batches, 1, n_features]\n",
        "\n",
        "    actor_loss = torch.sum((new_prob * (self.alpha * new_log_prob - new_q)), dim=-1, keepdim=True).mean()\n",
        "\n",
        "\n",
        "    self.actor_optimizer.zero_grad()\n",
        "    actor_loss.backward()\n",
        "    self.actor_optimizer.step()\n",
        "\n",
        "\n",
        "    # alpha loss and update\n",
        "\n",
        "\n",
        "    # selected actions log probabilities\n",
        "    selected_new_log_prob = torch.gather(new_log_prob, -1, new_action.unsqueeze(-1)) # shape[n_batches, 1, 1]\n",
        "\n",
        "\n",
        "    alpha_loss = torch.mean(-self.alpha * (selected_new_log_prob + self.target_entropy).detach())\n",
        "\n",
        "    self.alpha_optimizer.zero_grad()\n",
        "    alpha_loss.backward()\n",
        "    self.alpha_optimizer.step()\n",
        "\n",
        "\n",
        "    # target critics soft update\n",
        "\n",
        "    for param, target_param in zip(self.critic1.parameters(), self.target_critic1.parameters()):\n",
        "      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "    for param, target_param in zip(self.critic2.parameters(), self.target_critic2.parameters()):\n",
        "      target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
        "\n",
        "\n",
        "    # logs\n",
        "\n",
        "    dist = Categorical(probs=new_prob)\n",
        "    entropy = dist.entropy()\n",
        "    mean_entropy = round(torch.mean(entropy).item(), 3)\n",
        "    print(f'Mean entropy: {mean_entropy}')\n",
        "    print(f'Critic1 loss: {critic_loss1}\\t Critic2 loss: {critic_loss2}')\n",
        "\n",
        "    return critic_loss1, critic_loss2, mean_entropy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6KvlsT_oGjb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlB4TabfvlKN"
      },
      "source": [
        "#### **DQN class**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6VQQtbvwYdp"
      },
      "outputs": [],
      "source": [
        "class Q(nn.Module):\n",
        "\n",
        "  def __init__(self,in_dim=3, action_dim=3):\n",
        "    super().__init__()\n",
        "    self.in_dim = in_dim\n",
        "    self.action_dim = action_dim\n",
        "    self.linear_1 = nn.Linear(in_dim, 128)\n",
        "    self.linear_2 = nn.Linear(128, 64)\n",
        "    self.linear_3 = nn.Linear(64, 32)\n",
        "    self.linear_4 = nn.Linear(32, action_dim)\n",
        "  def forward(self, state):\n",
        "    x = F.relu(self.linear_1(state))\n",
        "    x = F.relu(self.linear_2(x))\n",
        "    x = F.relu(self.linear_3(x))\n",
        "    Q_values = F.relu(self.linear_4(x))\n",
        "\n",
        "    return Q_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CZEahmkY1Kga"
      },
      "outputs": [],
      "source": [
        "# state_sample [n_batches, 1, n_features]\n",
        "# actions_sample [n_batches, 1]\n",
        "# next_state_sample [n_batches, 1, n_features]\n",
        "# reward_sample [n_batches, 1, 1]\n",
        "# done_sample [n_batches, 1]\n",
        "# mean_rewards_whole_buffer -> scalar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KdntmweFvoPP"
      },
      "outputs": [],
      "source": [
        "class DQN:\n",
        "  def __init__(self, gamma=0.99, epsilon=1, exploration_decay_rate = 0.99, target_update_fc=10, min_exploration=0.10):\n",
        "\n",
        "    self.gamma = gamma\n",
        "    self.epsilon = epsilon\n",
        "    self.exploration_decay_rate = exploration_decay_rate\n",
        "    self.target_update_fc = target_update_fc\n",
        "    self.counter = 0\n",
        "    self.min_exploration = min_exploration\n",
        "\n",
        "    self.Q_online = Q()\n",
        "    self.Q_target = Q()\n",
        "\n",
        "    # Q-target should have the same parameters as the Q-online initially and periodically\n",
        "    self.Q_target.load_state_dict(self.Q_online.state_dict())\n",
        "\n",
        "    self.Q_online_optimizer = torch.optim.Adam(self.Q_online.parameters(), lr=1e-4)\n",
        "\n",
        "  def update_parameters(self, states, actions, next_states, rewards, dones):\n",
        "    # update epsilon\n",
        "    self.epsilon = max([self.epsilon * self.exploration_decay_rate, self.min_exploration])\n",
        "\n",
        "    # updating the counter to keep track of updating target network parameters\n",
        "    self.counter += 1\n",
        "\n",
        "    # target network computation\n",
        "    with torch.no_grad():\n",
        "      max_Q_target, _ = torch.max(self.Q_target(next_states), dim=-1) # output: [n_batches, 1]\n",
        "      target = rewards + (1 - dones.unsqueeze(-1)) * self.gamma * max_Q_target.unsqueeze(-1) # output: [n_batches, 1, 1]\n",
        "\n",
        "\n",
        "    # choose the Q values of the selected actions for the online network\n",
        "    q_values = self.Q_online(states)\n",
        "\n",
        "    selected_qs = torch.gather(q_values, -1, actions.unsqueeze(-1).unsqueeze(-1)) # output: [n_batches, 1, 1]\n",
        "\n",
        "    # use selected Qs and target values to compute loss\n",
        "    loss = F.mse_loss(selected_qs, target)\n",
        "\n",
        "    # update Q network parameters\n",
        "    self.Q_online_optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    self.Q_online_optimizer.step()\n",
        "\n",
        "\n",
        "    # check the counter to see if we need to update target network parameters\n",
        "    if(self.counter == self.target_update_fc):\n",
        "      self.Q_target.load_state_dict(self.Q_online.state_dict())\n",
        "      self.counter = 0\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBM3n6hCi8Cv"
      },
      "source": [
        "#### **Position tracker for test evaluation**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The position tracker is designed to track the performance of agent at every time stamp. Since it replicates how agent would perform in real-time environment, it is useful and essential. Features such as total, realized and unrealized PnL can be retrieved from this class.\n",
        "It is important to note that if positions are stacked (buy or sell) and one opposite action is chosen by agent, the whole position is closed."
      ],
      "metadata": {
        "id": "hwHLBPUlhegN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HtmD343zjDM6"
      },
      "outputs": [],
      "source": [
        "class PositionTracker:\n",
        "  def __init__(self):\n",
        "    self.entry_price = 0.0\n",
        "    self.realized_pnl = torch.zeros(1) # Ensure realized_pnl is a tensor\n",
        "    self.position_size = 0.0\n",
        "    self.position = 'Flat'\n",
        "    pass\n",
        "  def update(self, action, price_now):\n",
        "    if(action == 0): #buy\n",
        "      ask_price = price_now[1] # action on the ask\n",
        "      if(self.position == 'Flat'): # go from flat to long\n",
        "        self.position = 'Long'\n",
        "        self.position_size = 1\n",
        "        self.entry_price = ask_price\n",
        "      elif(self.position == 'Long'): #stack long positions\n",
        "        self.entry_price = (self.entry_price * self.position_size + ask_price * 1) / (self.position_size + 1)\n",
        "        self.position_size += 1\n",
        "      elif(self.position == 'Short'): # close the short positon\n",
        "        pnl = (self.entry_price - ask_price) * self.position_size\n",
        "        self.realized_pnl += pnl\n",
        "        self.entry_price = 0.0\n",
        "        self.position_size = 0.0\n",
        "        self.position = 'Flat'\n",
        "\n",
        "    elif(action == 2): #short\n",
        "      bid_price = price_now[0] #action on the bid\n",
        "      if (self.position == 'Flat'):\n",
        "        self.position = 'Short'\n",
        "        self.position_size = 1\n",
        "        self.entry_price = bid_price\n",
        "      elif(self.position == 'Short'):\n",
        "        self.entry_price = (self.entry_price * self.position_size + bid_price * 1) / (self.position_size + 1)\n",
        "        self.position_size += 1\n",
        "      elif(self.position == 'Long'): #close the long position\n",
        "        pnl = (bid_price - self.entry_price) * self.position_size\n",
        "        self.realized_pnl += pnl\n",
        "        self.entry_price = 0.0\n",
        "        self.position_size = 0.0\n",
        "        self.position = 'Flat'\n",
        "  def get_unrealized_pnl(self, price_now):\n",
        "    ask_price = price_now[1]\n",
        "    bid_price = price_now[0]\n",
        "    if (self.position == 'Long'):\n",
        "      # if you are long you look at the bid for unrealized pnl\n",
        "      return (bid_price - self.entry_price) * self.position_size\n",
        "    elif(self.position == 'Short'):\n",
        "      # If you are short you look at the ask for unrealized pnl (immediate buy back price)\n",
        "      return (self.entry_price - ask_price) * self.position_size\n",
        "    else:\n",
        "      return 0.0\n",
        "  def get_total_pnl(self, price_before):\n",
        "    return self.realized_pnl + self.get_unrealized_pnl(price_before)\n",
        "  def is_flat(self):\n",
        "    return self.position == 'Flat'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6pMIkKGFjQvY"
      },
      "source": [
        "#### **Test data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AvzJKJdjVn6"
      },
      "outputs": [],
      "source": [
        "test_states = state_func(orderbook_test_sac, ex_test_transformed_sac)\n",
        "len_test = test_states.size(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vH2IdCVqoZE2"
      },
      "source": [
        "#### **SAC Main body normalized total PnL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8vQq6EAyPF27"
      },
      "outputs": [],
      "source": [
        "episodes = 200\n",
        "\n",
        "env_len = 5000\n",
        "\n",
        "replay_buffer_SAC = ReplayBufferRaw(capacity=25000, env_len=env_len)\n",
        "\n",
        "\n",
        "n_rollout_environment = 1\n",
        "tot_data_len = orderbook_train_sac_nonzero.size(0)\n",
        "lambda_ = 0.01\n",
        "\n",
        "max_index = tot_data_len - env_len - 1\n",
        "sample_range = np.arange(max_index)\n",
        "\n",
        "entropy_list = []\n",
        "test_pnl_SAC = []\n",
        "test_pnl_SAC_realized = []\n",
        "\n",
        "sac = SAC(tau=0.005, actor_lr=1e-4, critics_lr=1e-4, alpha_lr=1e-4)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "for episode in range(episodes):\n",
        "\n",
        "  print(50 * '--')\n",
        "  print(f'\\nEpisode {episode}\\n')\n",
        "\n",
        "  dones = torch.zeros(n_rollout_environment)\n",
        "\n",
        "  sample_indices = np.random.choice(sample_range, size=n_rollout_environment)\n",
        "\n",
        "  position_tracker = PositionTracker()\n",
        "  for transition in range(env_len):\n",
        "\n",
        "    current_states = states[sample_indices]\n",
        "\n",
        "    actions, _, _ = sac.actor(current_states) # actions: [batches]\n",
        "\n",
        "    position_tracker.update(actions, prices_train_nonzero[transition])\n",
        "\n",
        "    next_states = states[sample_indices+1]\n",
        "\n",
        "\n",
        "    # compute rewards\n",
        "    reward = position_tracker.get_total_pnl(prices_train_nonzero[transition])\n",
        "\n",
        "\n",
        "    sample_indices += 1\n",
        "\n",
        "\n",
        "    if(transition + 1 == env_len):\n",
        "      dones = torch.ones(n_rollout_environment)\n",
        "\n",
        "    # store transition\n",
        "    replay_buffer_SAC.add(current_states, actions, next_states, reward, dones)\n",
        "\n",
        "\n",
        "\n",
        "  episode_mean_reward_SAC = replay_buffer_SAC.normalize_recent_rewards()\n",
        "\n",
        "\n",
        "  for each in range(3):\n",
        "    state_sample, actions_sample, next_state_sample, reward_sample, done_sample, mean_rewards_whole_buffer = replay_buffer_SAC.sample(n_batches=128)\n",
        "\n",
        "    c_loss_1, c_loss_2, entropy = sac.update_parameters(state_sample, actions_sample, next_state_sample, reward_sample, done_sample)\n",
        "\n",
        "\n",
        "\n",
        "    if(each == 0):\n",
        "      entropy_list.append(entropy)\n",
        "\n",
        "\n",
        "  # testing pnl on test data\n",
        "  if(episode % 10 == 0 or episode+1 == episodes):\n",
        "    print(50 * '**')\n",
        "    print(f\"\\nTest PnL\")\n",
        "    position_tracker = PositionTracker()\n",
        "\n",
        "    for t in range(len_test):\n",
        "      state = test_states[t]\n",
        "\n",
        "      actions, probs, log_probs = sac.actor(state)\n",
        "\n",
        "      best_action = torch.argmax(probs)\n",
        "\n",
        "      position_tracker.update(best_action, prices_test[t])\n",
        "\n",
        "    tot_pnl = position_tracker.get_total_pnl(prices_test[t]).item()\n",
        "    re_pnl = position_tracker.realized_pnl.item()\n",
        "    test_pnl_SAC.append(tot_pnl)\n",
        "    test_pnl_SAC_realized.append(re_pnl)\n",
        "    print(f\"Realized PnL: {re_pnl}\")\n",
        "    print(f\"Total PnL: {tot_pnl}\\n\")\n",
        "    print(50 * '**')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZgsu47qfJGf"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(0, 201, 10), test_pnl_SAC)\n",
        "plt.scatter(np.arange(0, 201, 10), test_pnl_SAC, alpha=0.3)\n",
        "plt.title('Test Data PnL with SAC')\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('PnL')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position_tracker = PositionTracker()\n",
        "test_time_pnl_list_total = []\n",
        "test_time_pnl_list_realized = []\n",
        "test_time_actions = []\n",
        "for t in range(len(test_states)):\n",
        "  state = test_states[t]\n",
        "\n",
        "  # actions, probs, log_probs = actor_copy(state)\n",
        "  actions, probs, log_probs = sac.actor(state)\n",
        "\n",
        "  best_action = torch.argmax(probs)\n",
        "\n",
        "  test_time_actions.append(best_action)\n",
        "\n",
        "  position_tracker.update(best_action, prices_test[t])\n",
        "  test_time_pnl_list_total.append(position_tracker.get_total_pnl(prices_test[t]).item())\n",
        "  test_time_pnl_list_realized.append(position_tracker.realized_pnl.item())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "I_oQMRz7pJjR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acs = torch.stack(test_time_actions)\n",
        "print(f\"Number of buy actions: {len(acs[acs == 0])}\")\n",
        "print(f\"Number of hold actions: {len(acs[acs == 1])}\")\n",
        "print(f\"Number of sell actions: {len(acs[acs == 2])}\")"
      ],
      "metadata": {
        "id": "oo4op1s-3nM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(test_time_pnl_list_total)), test_time_pnl_list_total)\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "Ns42uZ91X_i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIW8AY80CyGA"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(len(entropy_list)), entropy_list)\n",
        "plt.title('SAC Entropy Evolution')\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('entropy')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_tot_pnl_training_raw = test_pnl_SAC\n",
        "test_actions_raw = torch.stack(test_time_actions)\n",
        "entropy_list_raw = entropy_list\n",
        "actor_raw = Actor()\n",
        "actor_raw.load_state_dict(sac.actor_params())"
      ],
      "metadata": {
        "id": "XIWmTQZfS2lL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Re8CNOdUjQMt"
      },
      "source": [
        "#### **DQN Main body normalized total PnL**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "SHEbI23zjTR4"
      },
      "outputs": [],
      "source": [
        "episodes = 200\n",
        "# episodes = 2\n",
        "\n",
        "env_len = 5000 # not equal to capacity. After every \"buffer length\" times of data storage we update the DRL\n",
        "# env_len = 10\n",
        "\n",
        "replay_buffer_DQN = ReplayBufferRaw(capacity=25000, env_len=env_len)\n",
        "\n",
        "\n",
        "n_rollout_environment = 1\n",
        "tot_data_len = orderbook_train_sac_nonzero.size(0)\n",
        "lambda_ = 0.01\n",
        "\n",
        "max_index = tot_data_len - env_len - 1\n",
        "sample_range = np.arange(max_index)\n",
        "\n",
        "critic_losses_dqn = []\n",
        "exploration_list_dqn = []\n",
        "mean_rewards_DQN = []\n",
        "test_pnl_DQN = []\n",
        "\n",
        "dqn = DQN(exploration_decay_rate=0.98)\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "for episode in range(episodes):\n",
        "\n",
        "  print(50 * '--')\n",
        "  print(f'\\nEpisode {episode}\\n')\n",
        "\n",
        "  # reset done flag to zeros after each environment rollout is done\n",
        "  dones = torch.zeros(n_rollout_environment)\n",
        "\n",
        "  sample_indices = np.random.choice(sample_range, size=n_rollout_environment)\n",
        "\n",
        "\n",
        "  for transition in range(env_len):\n",
        "\n",
        "    current_states = states[sample_indices]\n",
        "\n",
        "    # retrive Qs for the states\n",
        "    qs_online = dqn.Q_online(current_states)\n",
        "\n",
        "    if np.random.rand() > dqn.epsilon:\n",
        "      # choose the max\n",
        "      actions = torch.argmax(qs_online)\n",
        "\n",
        "    else:\n",
        "      # random action\n",
        "      actions = torch.randint(0, 3, ())\n",
        "\n",
        "\n",
        "    # next_states = torch.concat([states[sample_indices+1], next_inventory], dim=-1)\n",
        "    next_states = states[sample_indices+1]\n",
        "\n",
        "\n",
        "    # compute rewards\n",
        "    reward = position_tracker.get_total_pnl(prices_train_nonzero[transition])\n",
        "\n",
        "\n",
        "    sample_indices += 1\n",
        "\n",
        "    if(transition + 1 == env_len):\n",
        "      dones = torch.ones(n_rollout_environment)\n",
        "\n",
        "\n",
        "    # store transition\n",
        "    replay_buffer_DQN.add(current_states, actions, next_states, reward, dones)\n",
        "\n",
        "\n",
        "  replay_buffer_DQN.normalize_recent_rewards()\n",
        "\n",
        "\n",
        "\n",
        "  for each in range(3):\n",
        "    state_sample, actions_sample, next_state_sample, reward_sample, done_sample, mean_rewards_whole_buffer = replay_buffer_DQN.sample(n_batches=128)\n",
        "\n",
        "    critic_loss = dqn.update_parameters(state_sample, actions_sample, next_state_sample, reward_sample, done_sample)\n",
        "\n",
        "    # store critic losses for the first sampling since the next updates have no effect on critic loss values\n",
        "    if(each == 0):\n",
        "      mean_rewards_DQN.append(mean_rewards_whole_buffer)\n",
        "      critic_losses_dqn.append(critic_loss)\n",
        "      exploration_list_dqn.append(dqn.epsilon)\n",
        "\n",
        "  # testing pnl on test data\n",
        "  if(episode % 10 == 0 or episode+1 == episodes):\n",
        "\n",
        "    with torch.no_grad():\n",
        "      print(50 * '**')\n",
        "      print(f\"Exploration: {dqn.epsilon}\")\n",
        "      print(f\"\\nTest PnL\")\n",
        "      position_tracker = PositionTracker()\n",
        "\n",
        "      for t in range(len_test):\n",
        "        state = test_states[t]\n",
        "\n",
        "        # in inference always choose the maximum Q's action\n",
        "        actions = torch.argmax(dqn.Q_online(state))\n",
        "\n",
        "        position_tracker.update(actions, prices_test[t])\n",
        "\n",
        "      tot_pnl = position_tracker.get_total_pnl(prices_test[t]).item()\n",
        "      test_pnl_DQN.append(tot_pnl)\n",
        "\n",
        "      print(f\"PnL: {tot_pnl}\\n\")\n",
        "      print(50 * '**')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# pnls\n",
        "plt.plot(np.arange(0, 201, 10), test_pnl_DQN)\n",
        "plt.scatter(np.arange(0, 201, 10), test_pnl_DQN, alpha=0.3)\n",
        "\n",
        "plt.title('DQN Test Data PnL')\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('PnL')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FmY8-kGRQMTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(exploration_list_dqn)\n",
        "plt.title('DQN Exploration Rate')\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('exploration rate')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y-wMvBqQQlFa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**comparison between DQN and SAC**"
      ],
      "metadata": {
        "id": "7Tox2fJ_hNzV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBvp_7kS4EzZ"
      },
      "outputs": [],
      "source": [
        "# pnls\n",
        "plt.plot(np.arange(0, 201, 10), test_pnl_DQN)\n",
        "plt.scatter(np.arange(0, 201, 10), test_pnl_DQN, alpha=0.3, label='DQN')\n",
        "\n",
        "plt.plot(np.arange(0, 201, 10), test_pnl_SAC, color='orange')\n",
        "plt.scatter(np.arange(0, 201, 10), test_pnl_SAC, color='orange', alpha=0.3, label='SAC')\n",
        "\n",
        "plt.title('Test Data PnL')\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('PnL')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLOwN-Jm7d07"
      },
      "source": [
        "#### **SAC Main body total PnL Sharpe reward**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "PW7iJpNj7d08"
      },
      "outputs": [],
      "source": [
        "\n",
        "episodes = 200\n",
        "\n",
        "env_len = 5000\n",
        "\n",
        "replay_buffer_SAC = ReplayBufferSharpe(capacity=25000, env_len=env_len)\n",
        "\n",
        "\n",
        "n_rollout_environment = 1\n",
        "tot_data_len = orderbook_train_sac_nonzero.size(0)\n",
        "\n",
        "max_index = tot_data_len - env_len - 1\n",
        "sample_range = np.arange(max_index)\n",
        "\n",
        "\n",
        "entropy_list = []\n",
        "test_pnl_SAC = []\n",
        "test_pnl_SAC_realized = []\n",
        "\n",
        "sac = SAC(tau=0.005, actor_lr=1e-4, critics_lr=1e-4, alpha_lr=1e-4)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "\n",
        "for episode in range(episodes):\n",
        "\n",
        "  print(50 * '--')\n",
        "  print(f'\\nEpisode {episode}\\n')\n",
        "\n",
        "  dones = torch.zeros(n_rollout_environment)\n",
        "\n",
        "  sample_indices = np.random.choice(sample_range, size=n_rollout_environment)\n",
        "\n",
        "  position_tracker = PositionTracker()\n",
        "\n",
        "  for transition in range(env_len):\n",
        "\n",
        "    current_states = states[sample_indices]\n",
        "\n",
        "    actions, _, _ = sac.actor(current_states) # actions: [n_batches]\n",
        "\n",
        "    # report the chosen action to position tracker\n",
        "    position_tracker.update(actions, prices_train_nonzero[transition])\n",
        "\n",
        "    next_states = states[sample_indices+1]\n",
        "\n",
        "    # compute rewards\n",
        "    reward = position_tracker.get_total_pnl(prices_train_nonzero[transition])\n",
        "\n",
        "    # update sample indices\n",
        "    sample_indices += 1\n",
        "\n",
        "    # if it is the last state in the evrionment, set done flag to true\n",
        "    if(transition + 1 == env_len):\n",
        "      dones = torch.ones(n_rollout_environment)\n",
        "\n",
        "    # store transition\n",
        "    replay_buffer_SAC.add(current_states, actions, next_states, reward, dones)\n",
        "\n",
        "\n",
        "  # after we add \"buffer_len\" more transitions to our buffer\n",
        "  # we sample N snapshots(state, action, next_state, reward) from the buffer and update parameters\n",
        "  # then we go back to rollout and capture transitions using our updated policy\n",
        "\n",
        "  # normalize rewards of the rollout which finished\n",
        "  episode_mean_reward_SAC = replay_buffer_SAC.normalize_recent_rewards()\n",
        "\n",
        "\n",
        "  # resample from the buffer and update multiple times (in this project 3) before going back to rollout\n",
        "\n",
        "  for each in range(3):\n",
        "    state_sample, actions_sample, next_state_sample, reward_sample, done_sample, mean_rewards_whole_buffer = replay_buffer_SAC.sample(n_batches=128)\n",
        "\n",
        "    c_loss_1, c_loss_2, entropy = sac.update_parameters(state_sample, actions_sample, next_state_sample, reward_sample, done_sample)\n",
        "\n",
        "\n",
        "    # store entropy for the first sampling\n",
        "    if(each == 0):\n",
        "      entropy_list.append(entropy)\n",
        "\n",
        "\n",
        "  # agent evaluation on test set every 10 episodes and every 30 updates of the parameters\n",
        "\n",
        "  if(episode % 10 == 0 or episode+1 == episodes):\n",
        "    print(50 * '**')\n",
        "    print(f\"\\nTest PnL\")\n",
        "    position_tracker = PositionTracker()\n",
        "\n",
        "    for t in range(len_test):\n",
        "      state = test_states[t]\n",
        "\n",
        "      actions, probs, log_probs = sac.actor(state)\n",
        "\n",
        "      # choosing the action with the highest probability\n",
        "      best_action = torch.argmax(probs)\n",
        "\n",
        "      position_tracker.update(best_action, prices_test[t])\n",
        "\n",
        "    tot_pnl = position_tracker.get_total_pnl(prices_test[t]).item()\n",
        "    re_pnl = position_tracker.realized_pnl.item()\n",
        "    test_pnl_SAC.append(tot_pnl)\n",
        "    test_pnl_SAC_realized.append(re_pnl)\n",
        "    print(f\"Realized PnL: {re_pnl}\")\n",
        "    print(f\"Total PnL: {tot_pnl}\\n\")\n",
        "    print(50 * '**')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How the final total PnL changes through training**"
      ],
      "metadata": {
        "id": "6n0S2keLZ0W7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B4gJxzld7d0-"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(0, 201, 10), test_pnl_SAC)\n",
        "plt.scatter(np.arange(0, 201, 10), test_pnl_SAC, alpha=0.3)\n",
        "plt.title('Test Data Total PnL with SAC')\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('PnL')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How PnL changes as the final agent goes through the test data**"
      ],
      "metadata": {
        "id": "m2Tt4LYMZ8vy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "position_tracker = PositionTracker()\n",
        "test_time_pnl_list_total = []\n",
        "test_time_pnl_list_realized = []\n",
        "test_time_actions = []\n",
        "for t in range(len(test_states)):\n",
        "\n",
        "  state = test_states[t]\n",
        "\n",
        "  # actions, probs, log_probs = actor_copy(state)\n",
        "  actions, probs, log_probs = sac.actor(state)\n",
        "\n",
        "  best_action = torch.argmax(probs)\n",
        "\n",
        "  test_time_actions.append(best_action)\n",
        "\n",
        "  position_tracker.update(best_action, prices_test[t])\n",
        "  test_time_pnl_list_total.append(position_tracker.get_total_pnl(prices_test[t]).item())\n",
        "  test_time_pnl_list_realized.append(position_tracker.realized_pnl.item())"
      ],
      "metadata": {
        "collapsed": true,
        "id": "a9X9Ji3u7d09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acs = torch.stack(test_time_actions)\n",
        "print(f\"Number of buy actions: {len(acs[acs == 0])}\")\n",
        "print(f\"Number of hold actions: {len(acs[acs == 1])}\")\n",
        "print(f\"Number of sell actions: {len(acs[acs == 2])}\")"
      ],
      "metadata": {
        "id": "EGxlV6Jb7d09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(test_time_pnl_list_total)), test_time_pnl_list_total)\n",
        "plt.title('Total PnL Change on Test Set')\n",
        "plt.xlabel('time')\n",
        "plt.ylabel('total PnL')\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "4873a4HG7d09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xGF9BAM07d0_"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(len(entropy_list)), entropy_list)\n",
        "plt.title('SAC Entropy Evolution')\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('entropy')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_tot_pnl_training_sharpe = test_pnl_SAC\n",
        "test_actions_sharpe = torch.stack(test_time_actions)\n",
        "entropy_list_sharpe = entropy_list\n",
        "actor_sharpe = Actor()\n",
        "actor_sharpe.load_state_dict(sac.actor_params())"
      ],
      "metadata": {
        "id": "BBkh5KzZgkB5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.concat([test_actions_raw, test_actions_sharpe], dim=0)\n",
        "labels=['raw', 'sharpe']\n",
        "both = torch.concat([test_actions_raw.unsqueeze(-1), test_actions_sharpe.unsqueeze(-1)], dim=-1).numpy()\n",
        "plt.hist(both, label=labels, bins=[-0.5, 0.5, 1.5, 2.5], rwidth=0.2)\n",
        "# plt.xticks(['buy', 'hold', 'sell'])\n",
        "plt.xlabel('action')\n",
        "plt.ylabel('frequency')\n",
        "plt.grid(alpha=0.75)\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "zcEphi4gXP2b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdvTMpsAizVs"
      },
      "source": [
        "#### **SAC Main body realized PnL Sharpe reward**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aacwz2zVizVt"
      },
      "outputs": [],
      "source": [
        "episodes = 200\n",
        "\n",
        "max_inventory = 5000\n",
        "\n",
        "env_len = 5000\n",
        "\n",
        "replay_buffer_SAC = ReplayBufferSharpe(capacity=25000, env_len=env_len)\n",
        "\n",
        "\n",
        "n_rollout_environment = 1\n",
        "tot_data_len = orderbook_train_sac_nonzero.size(0)\n",
        "lambda_ = 0.01\n",
        "\n",
        "max_index = tot_data_len - env_len - 1\n",
        "sample_range = np.arange(max_index)\n",
        "\n",
        "\n",
        "entropy_list = []\n",
        "test_pnl_SAC = []\n",
        "test_pnl_SAC_realized = []\n",
        "\n",
        "sac = SAC(tau=0.005, actor_lr=1e-4, critics_lr=1e-4, alpha_lr=1e-4)\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "for episode in range(episodes):\n",
        "\n",
        "  print(50 * '--')\n",
        "  print(f'\\nEpisode {episode}\\n')\n",
        "\n",
        "  dones = torch.zeros(n_rollout_environment)\n",
        "\n",
        "\n",
        "  sample_indices = np.random.choice(sample_range, size=n_rollout_environment)\n",
        "\n",
        "\n",
        "  position_tracker = PositionTracker()\n",
        "  for transition in range(env_len):\n",
        "\n",
        "    current_states = states[sample_indices]\n",
        "\n",
        "    actions, _, _ = sac.actor(current_states) # actions: [batches]\n",
        "\n",
        "\n",
        "    # keep position tracker updated about the chosen action\n",
        "    position_tracker.update(actions, prices_train_nonzero[transition])\n",
        "\n",
        "\n",
        "    # next_states = torch.concat([states[sample_indices+1], next_inventory], dim=-1)\n",
        "    next_states = states[sample_indices+1]\n",
        "\n",
        "\n",
        "    # compute rewards (using realized PnL only)\n",
        "\n",
        "    reward = position_tracker.realized_pnl\n",
        "\n",
        "\n",
        "    sample_indices += 1\n",
        "\n",
        "    if(transition + 1 == env_len):\n",
        "      dones = torch.ones(n_rollout_environment)\n",
        "\n",
        "\n",
        "    # store transition\n",
        "    replay_buffer_SAC.add(current_states, actions, next_states, reward, dones)\n",
        "\n",
        "\n",
        "  episode_mean_reward_SAC = replay_buffer_SAC.normalize_recent_rewards()\n",
        "\n",
        "  print(f'Episode mean reward: {episode_mean_reward_SAC}')\n",
        "\n",
        "  for each in range(3):\n",
        "    state_sample, actions_sample, next_state_sample, reward_sample, done_sample, mean_rewards_whole_buffer = replay_buffer_SAC.sample(n_batches=128)\n",
        "\n",
        "    c_loss_1, c_loss_2, entropy = sac.update_parameters(state_sample, actions_sample, next_state_sample, reward_sample, done_sample)\n",
        "\n",
        "\n",
        "    if(each == 0):\n",
        "      entropy_list.append(entropy)\n",
        "\n",
        "\n",
        "  # testing pnl on test data\n",
        "  if(episode % 10 == 0 or episode+1 == episodes):\n",
        "    print(50 * '**')\n",
        "    print(f\"\\nTest PnL\")\n",
        "    # position_tracker = PositonTracker()\n",
        "    position_tracker = PositionTracker()\n",
        "\n",
        "    for t in range(len_test):\n",
        "      state = test_states[t]\n",
        "\n",
        "      actions, probs, log_probs = sac.actor(state)\n",
        "\n",
        "      best_action = torch.argmax(probs)\n",
        "\n",
        "      position_tracker.update(best_action, prices_test[t])\n",
        "\n",
        "    tot_pnl = position_tracker.get_total_pnl(prices_test[t]).item()\n",
        "    re_pnl = position_tracker.realized_pnl.item()\n",
        "    test_pnl_SAC.append(tot_pnl)\n",
        "    test_pnl_SAC_realized.append(re_pnl)\n",
        "    print(f\"Realized PnL: {re_pnl}\")\n",
        "    print(f\"Total PnL: {tot_pnl}\\n\")\n",
        "    print(50 * '**')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**How the final test set realized and total PnL evolved**"
      ],
      "metadata": {
        "id": "KRX01ndOAnei"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "peK_YHvzizVv"
      },
      "outputs": [],
      "source": [
        "plt.plot(np.arange(0, 201, 10), test_pnl_SAC)\n",
        "plt.scatter(np.arange(0, 201, 10), test_pnl_SAC, alpha=0.3, label='total PnL')\n",
        "plt.plot(np.arange(0, 201, 10), test_pnl_SAC_realized)\n",
        "plt.scatter(np.arange(0, 201, 10), test_pnl_SAC_realized, alpha=0.3, label='realized PnL')\n",
        "plt.title('Test Dataset Total and Realized PnL')\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('PnL')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "position_tracker = PositionTracker()\n",
        "test_time_pnl_list_total = []\n",
        "test_time_pnl_list_realized = []\n",
        "test_time_actions = []\n",
        "position_size_list = []\n",
        "for t in range(len(test_states)):\n",
        "  print(round(t/len(test_states), 3))\n",
        "  state = test_states[t]\n",
        "\n",
        "  # actions, probs, log_probs = actor_copy(state)\n",
        "  actions, probs, log_probs = sac.actor(state)\n",
        "\n",
        "  best_action = torch.argmax(probs)\n",
        "\n",
        "  test_time_actions.append(best_action)\n",
        "\n",
        "  position_tracker.update(best_action, prices_test[t])\n",
        "  test_time_pnl_list_total.append(position_tracker.get_total_pnl(prices_test[t]).item())\n",
        "  test_time_pnl_list_realized.append(position_tracker.realized_pnl.item())\n",
        "  position_size_list.append(position_tracker.position_size)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Le3abNG1izVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acs = torch.stack(test_time_actions)\n",
        "print(len(acs[acs == 0]))\n",
        "print(len(acs[acs == 1]))\n",
        "print(len(acs[acs == 2]))"
      ],
      "metadata": {
        "id": "OXik56B9izVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(mid_prices_test)\n",
        "for i in torch.where(acs == 2)[0].numpy():\n",
        "  plt.axvline(i, color='r', alpha=0.2)\n",
        "plt.xlabel('time')\n",
        "plt.ylabel('price')\n",
        "# plt.title('Sell Actions on Test Set')\n",
        "plt.grid(alpha=0.2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fpfyGrcJc0ON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(test_time_pnl_list_total)), test_time_pnl_list_total, color='#87CEFA', label='total PnL')\n",
        "plt.plot(range(len(test_time_pnl_list_realized)), test_time_pnl_list_realized, color='#4682B4', label='realized PnL')\n",
        "# for i in torch.where(acs == 2)[0].numpy():\n",
        "#   plt.axvline(i, color='r', alpha=0.2)\n",
        "# plt.title('Total and Realized Equity Curve Throughout Test Data ')\n",
        "plt.xlabel('time')\n",
        "plt.ylabel('PnL')\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "keAfiwE-izVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(len(test_time_pnl_list_total)), test_time_pnl_list_total, color='#200047', label='total PnL')\n",
        "plt.plot(range(len(test_time_pnl_list_realized)), test_time_pnl_list_realized, color='#d7aa5b', label='realized PnL')\n",
        "# for i in torch.where(acs == 2)[0].numpy():\n",
        "#   plt.axvline(i, color='r', alpha=0.2)\n",
        "# plt.title('Total and Realized Equity Curve Throughout Test Data ')\n",
        "plt.xlabel('time')\n",
        "plt.ylabel('PnL')\n",
        "plt.legend()\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "pUxxye0Nj3Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(abs(np.array(test_time_pnl_list_total) - np.array(test_time_pnl_list_realized)), color='#200047')\n",
        "plt.xlabel('time')\n",
        "plt.ylabel('discrepancy')\n",
        "plt.title('Total and Realized PnL Discrepancy')\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "2CNFK8G5_t53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(position_size_list, color='#200047')\n",
        "plt.xlabel('time')\n",
        "plt.ylabel('position size')\n",
        "plt.grid(True)"
      ],
      "metadata": {
        "id": "sGni8n-X6ABx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"total pnl Sharpe ratio: {np.mean(test_time_pnl_list_total)/ np.std(test_time_pnl_list_total)}\")\n",
        "print(f\"realized pnl Sharpe ratio: {np.mean(test_time_pnl_list_realized)/ np.std(test_time_pnl_list_realized)}\")"
      ],
      "metadata": {
        "id": "3dtHlqephS4m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"final total pnl: {test_time_pnl_list_total[-1]}\")\n",
        "print(f\"final realized pnl: {test_time_pnl_list_realized[-1]}\")\n",
        "print(f\"biggest total drawdown: {np.min(test_time_pnl_list_total)}\")\n",
        "print(f\"biggest realized drawdown: {np.min(test_time_pnl_list_realized)}\")\n",
        "print(f\"biggest total profit: {np.max(test_time_pnl_list_total)}\")\n",
        "print(f\"biggest realized profit: {np.max(test_time_pnl_list_realized)}\")"
      ],
      "metadata": {
        "id": "9Ce_DBBCgj9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(acs, bins=[-0.5, 0.5, 1.5, 2.5], rwidth=0.2, color='#132952')\n",
        "\n",
        "plt.title('Distribution of Actions')\n",
        "plt.xlabel('action')\n",
        "plt.ylabel('frequency')\n",
        "plt.grid(alpha=0.75)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_FMN_W3GizVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = np.array(test_pnl_SAC_realized) - np.array(test_pnl_SAC)"
      ],
      "metadata": {
        "id": "8raeZWaWBtis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = px.histogram(a,\n",
        "                   nbins=13,\n",
        "                   color_discrete_sequence=['#141757'])\n",
        "fig.update_layout(bargap=0.5)\n",
        "# fig.update_layout(template=\"plotly_white\")\n",
        "fig.update_xaxes(title_text=\"discrepancy\")\n",
        "fig.update_yaxes(title_text=\"frequency\", hoverformat=\".3f\")\n",
        "\n",
        "# Configure other layout properties\n",
        "fig.update_layout(\n",
        "    # title_text=\"Discrepancy\",\n",
        "    height=500,\n",
        "    width=900,\n",
        "    template=\"plotly_white\",\n",
        "    showlegend=False\n",
        ")\n",
        "fig.update_xaxes(showline=True, linewidth=1, linecolor='black', mirror=True)\n",
        "fig.update_yaxes(showline=True, linewidth=1, linecolor='black', mirror=True)\n",
        "fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
        "fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')\n",
        "fig.update_layout(\n",
        "    autosize=False,\n",
        "    width=640,\n",
        "    height=480,\n",
        ")"
      ],
      "metadata": {
        "id": "-Lo4W11VBuwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoJWe_lyizVv"
      },
      "outputs": [],
      "source": [
        "plt.plot(range(len(entropy_list)), entropy_list)\n",
        "plt.title('SAC Entropy Evolution')\n",
        "plt.xlabel('episode')\n",
        "plt.ylabel('entropy')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "2Ld1aTXwXSM_",
        "z334_YXs8x2p",
        "der6WbRYds3I",
        "qKMrpFrKaMkQ",
        "vqffespltvRP",
        "zdzLaD-4aUGj",
        "4TiQ4XEN1_YL",
        "9ZQ1Bmi9oOAb",
        "mlB4TabfvlKN",
        "ZBM3n6hCi8Cv",
        "6pMIkKGFjQvY",
        "vH2IdCVqoZE2",
        "Re8CNOdUjQMt",
        "dLOwN-Jm7d07"
      ],
      "provenance": [],
      "mount_file_id": "13_OhnOsqCwpg3TC-voZtb_9Ys0FTTrNd",
      "authorship_tag": "ABX9TyOzhYHlksdXh4Ly2Q3+gtPW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}